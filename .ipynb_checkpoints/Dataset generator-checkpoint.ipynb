{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nfrom scipy.sparse import csr_matrix, hstack, save_npz"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": "def preprocessingNews(toTrainData, toTestData):\n\tdef normalizeData(X):\n\t\toffset = np.mean(X, 0)\n\t\tscale = np.std(X, 0).clip(min=1)\n\t\tX = (X - offset) / scale\n\t\tX = X.astype(np.float32)\t\n\t\treturn X \n\treturn normalizeData(toTrainData),normalizeData(toTestData)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def shuffleAndSplitData(dataX, dataY,cluster):\n\tc = zip(dataX, dataY)\n\trandom.shuffle(c)\n\tdataX, dataY = zip(*c)\n\ttoTrainData  = np.array(dataX[:cluster])\n\ttoTrainLabel = np.array(dataY[:cluster])\n\t\n\tshadowData  = np.array(dataX[cluster:cluster*2])\n\tshadowLabel = np.array(dataY[cluster:cluster*2])\n\t\n\ttoTestData  = np.array(dataX[cluster*2:cluster*3])\n\ttoTestLabel = np.array(dataY[cluster*2:cluster*3])\n\t\n\tshadowTestData  = np.array(dataX[cluster*3:cluster*4])\n\tshadowTestLabel = np.array(dataY[cluster*3:cluster*4])\n\n\treturn toTrainData, toTrainLabel,shadowData,shadowLabel,toTestData,toTestLabel,shadowTestData,shadowTestLabel"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": "dataPath=\".\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "newsgroups_train = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes')  )\nnewsgroups_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes') )\nX = np.concatenate((newsgroups_train.data , newsgroups_test.data), axis=0)\ny = np.concatenate((newsgroups_train.target , newsgroups_test.target), axis=0)\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(X)\nX = X.toarray()\nprint(\"Preprocessing data\")\nprint(X.shape)\ncluster = 4500\ndataPath = dataFolderPath+dataset+'/Preprocessed'\ntoTrainData, toTrainLabel,shadowData,shadowLabel,toTestData,toTestLabel,shadowTestData,shadowTestLabel = shuffleAndSplitData(X, y,cluster)\ntoTrainDataSave, toTestDataSave    = preprocessingNews(toTrainData, toTestData)\nshadowDataSave, shadowTestDataSave = preprocessingNews(shadowData, shadowTestData)\nnp.savez(dataPath + '/targetTrain.npz', toTrainDataSave, toTrainLabel)\nnp.savez(dataPath + '/targetTest.npz',  toTestDataSave, toTestLabel)\nnp.savez(dataPath + '/shadowTrain.npz', shadowDataSave, shadowLabel)\nnp.savez(dataPath + '/shadowTest.npz',  shadowTestDataSave, shadowTestLabel)\n\nprint(\"Preprocessing finished\\n\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": "# y=csr_matrix(y)"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": "# dataset_total=(hstack((X,y.transpose()))).tocsr()"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(18846, 130108)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "# dataset_total.shape"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": "# size=int(dataset_total.shape[0]*0.25)"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(4711, 130108) (4711, 130108) (4711, 130108) (4713, 130108)\n"
    }
   ],
   "source": "# set1=dataset_total[:size,:]\n# set2=dataset_total[size:2*size, :]\n# set3=dataset_total[2*size:3*size, :]\n# set4=dataset_total[3*size:, :]\n# print(set1.shape, set2.shape, set3.shape, set4.shape)"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": "# df1=pd.DataFrame(set1)\n# df1.to_csv(\"set1.csv\", index=False, header=True)\n\n# df2=pd.DataFrame(set2)\n# df2.to_csv(\"set2.csv\", index=False, header=True)\n\n# df3=pd.DataFrame(set3)\n# df3.to_csv(\"set3.csv\", index=False, header=True)\n\n# df4=pd.DataFrame(set4)\n# df4.to_csv(\"set4.csv\", index=False, header=True)"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": "# save_npz(\"set1.npz\", set1)\n# save_npz(\"set2.npz\", set1)\n# save_npz(\"set3.npz\", set1)\n# save_npz(\"set4.npz\", set1)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
